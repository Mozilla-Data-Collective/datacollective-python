{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Mozilla Data Collective Python SDK Library","text":"<p>Welcome to the documentation for the <code>datacollective</code> Python client for the Mozilla Data Collective REST API.</p> <p>This library helps you:</p> <ul> <li>Authenticate with the Mozilla Data Collective.</li> <li>Download datasets to local storage.</li> <li>Load supported datasets into AI-friendly formats, such as pandas DataFrames.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install datacollective\n</code></pre>"},{"location":"#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the Mozilla Data Collective API, you need an API key:</p> <ol> <li>Sign up to the Mozilla Data Collective platform.</li> <li>Create or retrieve an API key from your Account -&gt; Credentials page.</li> <li>Store your key secret in a <code>.env</code> file and do not commit it to version control (git).</li> </ol>"},{"location":"#configuration","title":"Configuration","text":"<p>The client reads configuration from environment variables and <code>.env</code> files.</p>"},{"location":"#environment-variables","title":"Environment variables","text":"<p>Required:</p> <ul> <li><code>MDC_API_KEY</code> - Your Mozilla Data Collective API key.</li> </ul> <p>Optional:</p> <ul> <li><code>MDC_API_URL</code> - API endpoint (defaults to the production URL).</li> <li><code>MDC_DOWNLOAD_PATH</code> - Local directory where datasets will be downloaded   (defaults to <code>~/.mozdata/datasets</code>).</li> </ul> <p>Example using environment variables directly:</p> <pre><code>export MDC_API_KEY=your-api-key-here\nexport MDC_API_URL=https://datacollective.mozillafoundation.org/api\nexport MDC_DOWNLOAD_PATH=~/.mozdata/datasets\n</code></pre>"},{"location":"#env-file","title":"<code>.env</code> file","text":"<p>The client will automatically load configuration from a <code>.env</code> file in your project root or present working directory.</p> <p>Create a file named <code>.env</code>:</p> <pre><code># MDC API Configuration\nMDC_API_KEY=your-api-key-here\nMDC_API_URL=https://datacollective.mozillafoundation.org/api\nMDC_DOWNLOAD_PATH=~/.mozdata/datasets\n</code></pre> <p>Security note: do not commit <code>.env</code> files to version control, as they contain secrets.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>IMPORTANT NOTE: Before trying to access any dataset, make sure you have thoroughly read and agreed to the specific dataset's conditions &amp; licensing terms.</p> <p>[!TIP] You can find the <code>dataset-id</code> by looking at the URL of the dataset's page on MDC platform. The ID is the unique string of characters located at the very end of the URL, after the <code>/datasets/</code> path. For example, for URL <code>https://datacollective.mozillafoundation.org/datasets/cmflnuzw6lrt9e6ui4kwcshvn</code> dataset id will be <code>cmflnuzw6lrt9e6ui4kwcshvn</code>.</p>"},{"location":"#download-a-dataset","title":"Download a dataset","text":"<p>Use <code>save_dataset_to_disk</code> to download a dataset to the configured download path:</p> <pre><code>from datacollective import save_dataset_to_disk\n\ndataset = save_dataset_to_disk(\"your-dataset-id\")\n\n# Depending on the implementation, `dataset` may contain metadata\n# about the downloaded files or a higher-level dataset object.\n</code></pre> <p>The files will be stored under <code>MDC_DOWNLOAD_PATH</code> (default <code>~/.mozdata/datasets</code>).</p>"},{"location":"#loading-and-querying-datasets","title":"Loading and Querying Datasets","text":"<p>Note: in-memory dataset loading is currently supported only for certain datasets.</p> <p>You can load supported datasets into memory and convert them to a <code>pandas</code> <code>DataFrame</code> for analysis:</p> <pre><code>from datacollective import load_dataset\n\ndataset = load_dataset(\"your-dataset-id\")\n\n# Convert to pandas\ndf = dataset.to_pandas()\n\n# Inspect available splits (e.g., train, dev, test)\nprint(dataset.splits)\n</code></pre> <p>Once loaded into a <code>DataFrame</code>, you can use standard <code>pandas</code> operations to filter, aggregate, and analyze the data.</p>"},{"location":"#get-dataset-details","title":"Get dataset details","text":"<p>You can retrieve info from the datasheet of a dataset without downloading it:</p> <pre><code>from datacollective import get_dataset_details\n\ninfo = get_dataset_details(\"your-dataset-id\")\nprint(info)\n</code></pre>"},{"location":"#automatic-download-resume","title":"Automatic Download Resume","text":"<p>The SDK automatically handles interrupted downloads. If a download is interrupted for any reason (network error, user cancellation, system shutdown, etc.), the SDK will automatically resume from where it left off when you call <code>save_dataset_to_disk</code> or <code>load_dataset</code> again.</p> <p>How it works:</p> <ol> <li>When a download starts, the SDK creates a <code>.checksum</code> file alongside the partial    download (<code>.part</code> file) to track the download state.</li> <li>If the download is interrupted, both files are preserved.</li> <li>On the next download attempt, the SDK detects the partial download and resumes    from the last byte received.</li> <li>Once the download completes successfully, the temporary files are automatically    cleaned up.</li> </ol> <p>[!TIP] You don't need to do anything special to enable resume functionality, it works automatically. Just call the same function again after an interruption.</p> <p>Edge cases handled:</p> <ul> <li>If the dataset has been updated since the interrupted download, the SDK detects   the checksum mismatch and starts a fresh download.</li> <li>If only partial files exist without proper tracking data, the SDK safely starts   a fresh download.</li> </ul>"},{"location":"#automatically-check-for-extracted-archives","title":"Automatically check for extracted archives","text":"<p>The <code>load_dataset</code> function avoids redundant extraction by automatically detecting existing files.  It checks if the dataset archive is already downloaded and the folder is extracted under the same name.  This behavior applies when <code>overwrite_existing=False</code> and <code>overwrite_extracted=False</code>.  The SDK identifies the data if the extracted folder name matches the archive name without the extension.</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>For a detailed API reference, see the API Reference section of the documentation.</p> <p>[!NOTE] This section is intended for maintainers of the <code>datacollective</code> library.</p>"},{"location":"#tests","title":"Tests","text":"<p>Run the full test suite: <pre><code>pytest -v\n</code></pre></p> <p>Note that the e2e tests require a valid <code>MDC_TEST_API_KEY</code> and a <code>MDC_TEST_API_URL</code> key set in your environment. Pytest will skip the live e2e tests automatically if either is missing.</p>"},{"location":"#release-workflow","title":"Release Workflow","text":"<p>Check out the Release Workflow document for details on how to publish new versions of the library to PyPI using GitHub Actions.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#datacollective.datasets","title":"<code>datacollective.datasets</code>","text":""},{"location":"api/#datacollective.datasets.get_dataset_details","title":"<code>get_dataset_details(dataset_id)</code>","text":"<p>Return dataset details from the MDC API as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The dataset ID (as shown in MDC platform).</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dict with dataset details as returned by the API.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_id is empty.</p> <code>FileNotFoundError</code> <p>If the dataset does not exist (404).</p> <code>PermissionError</code> <p>If access is denied (403).</p> <code>RuntimeError</code> <p>If rate limit is exceeded (429).</p> <code>HTTPError</code> <p>For other non-2xx responses.</p> Source code in <code>src/datacollective/datasets.py</code> <pre><code>def get_dataset_details(dataset_id: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Return dataset details from the MDC API as a dictionary.\n\n    Args:\n        dataset_id: The dataset ID (as shown in MDC platform).\n\n    Returns:\n        A dict with dataset details as returned by the API.\n\n    Raises:\n        ValueError: If dataset_id is empty.\n        FileNotFoundError: If the dataset does not exist (404).\n        PermissionError: If access is denied (403).\n        RuntimeError: If rate limit is exceeded (429).\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    if not dataset_id or not dataset_id.strip():\n        raise ValueError(\"`dataset_id` must be a non-empty string\")\n\n    url = f\"{_get_api_url()}/datasets/{dataset_id}\"\n    resp = send_api_request(method=\"GET\", url=url)\n    return dict(resp.json())\n</code></pre>"},{"location":"api/#datacollective.datasets.load_dataset","title":"<code>load_dataset(dataset_id, download_directory=None, show_progress=True, overwrite_existing=False, overwrite_extracted=False)</code>","text":"<p>Download (if needed), extract (if not already extracted), and load the dataset into a pandas DataFrame.</p> <p>If the dataset archive already exists in the download directory, it will not be re-downloaded unless <code>overwrite_existing=True</code>.</p> <p>If there is a directory with the same name as the archive file without the suffix extension, we assume it has already been extracted, and it will not be re-extracted unless <code>overwrite_extracted=True</code>.</p> <p>Uses dataset <code>details['name']</code> to check in registry.py for dataset-specific loading logic.</p> <p>Automatically resumes interrupted downloads if a .checksum file exists from a previous attempt.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The dataset ID (as shown in MDC platform).</p> required <code>download_directory</code> <code>str | None</code> <p>Directory where to save the downloaded archive file. If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar during download.</p> <code>True</code> <code>overwrite_existing</code> <code>bool</code> <p>Whether to overwrite existing archive.</p> <code>False</code> <code>overwrite_extracted</code> <code>bool</code> <p>Whether to overwrite existing extracted files by re-extracting the archive file. Only makes sense when overwrite_existing is False. Will check in the download directory for existing extracted files with the default naming of the folder.</p> <code>False</code> <p>Returns:     A pandas DataFrame with the loaded dataset.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_id is empty.</p> <code>FileNotFoundError</code> <p>If the dataset does not exist (404).</p> <code>PermissionError</code> <p>If access is denied (403) or download directory is not writable.</p> <code>RuntimeError</code> <p>If rate limit is exceeded (429) or unexpected response format.</p> <code>HTTPError</code> <p>For other non-2xx responses.</p> Source code in <code>src/datacollective/datasets.py</code> <pre><code>def load_dataset(\n    dataset_id: str,\n    download_directory: str | None = None,\n    show_progress: bool = True,\n    overwrite_existing: bool = False,\n    overwrite_extracted: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Download (if needed), extract (if not already extracted), and load the dataset into a pandas DataFrame.\n\n    If the dataset archive already exists in the download directory, it will not be re-downloaded\n    unless `overwrite_existing=True`.\n\n    If there is a directory with the same name as the archive file without the suffix extension, we assume\n    it has already been extracted, and it will not be re-extracted unless `overwrite_extracted=True`.\n\n    Uses dataset `details['name']` to check in registry.py for dataset-specific loading logic.\n\n    Automatically resumes interrupted downloads if a .checksum file exists from a\n    previous attempt.\n\n    Args:\n        dataset_id: The dataset ID (as shown in MDC platform).\n        download_directory: Directory where to save the downloaded archive file.\n            If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.\n        show_progress: Whether to show a progress bar during download.\n        overwrite_existing: Whether to overwrite existing archive.\n        overwrite_extracted: Whether to overwrite existing extracted files by re-extracting the archive file.\n            Only makes sense when overwrite_existing is False.\n            Will check in the download directory for existing extracted files with the default naming of the folder.\n    Returns:\n        A pandas DataFrame with the loaded dataset.\n\n    Raises:\n        ValueError: If dataset_id is empty.\n        FileNotFoundError: If the dataset does not exist (404).\n        PermissionError: If access is denied (403) or download directory is not writable.\n        RuntimeError: If rate limit is exceeded (429) or unexpected response format.\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    archive_path = save_dataset_to_disk(\n        dataset_id=dataset_id,\n        download_directory=download_directory,\n        show_progress=show_progress,\n        overwrite_existing=overwrite_existing,\n    )\n    base_dir = resolve_download_dir(download_directory)\n    extract_dir = _extract_archive(\n        archive_path=archive_path,\n        dest_dir=base_dir,\n        overwrite_extracted=overwrite_extracted,\n    )\n\n    details = get_dataset_details(dataset_id)\n    dataset_name = str(details.get(\"name\", \"\")).lower()\n\n    return load_dataset_from_name_as_dataframe(dataset_name, extract_dir)\n</code></pre>"},{"location":"api/#datacollective.datasets.save_dataset_to_disk","title":"<code>save_dataset_to_disk(dataset_id, download_directory=None, show_progress=True, overwrite_existing=False)</code>","text":"<p>Download the dataset archive to a local directory and return the archive path. Skips download if the target file already exists (unless <code>overwrite_existing=True</code>).</p> <p>Automatically resumes interrupted downloads if a matching .checksum file exists from a previous attempt.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The dataset ID (as shown in MDC platform).</p> required <code>download_directory</code> <code>str | None</code> <p>Directory where to save the downloaded archive file. If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.</p> <code>None</code> <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar during download.</p> <code>True</code> <code>overwrite_existing</code> <code>bool</code> <p>Whether to overwrite the existing archive file.</p> <code>False</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the downloaded dataset archive.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_id is empty.</p> <code>FileNotFoundError</code> <p>If the dataset does not exist (404).</p> <code>PermissionError</code> <p>If access is denied (403) or download directory is not writable.</p> <code>RuntimeError</code> <p>If rate limit is exceeded (429) or unexpected response format.</p> <code>HTTPError</code> <p>For other non-2xx responses.</p> Source code in <code>src/datacollective/datasets.py</code> <pre><code>def save_dataset_to_disk(\n    dataset_id: str,\n    download_directory: str | None = None,\n    show_progress: bool = True,\n    overwrite_existing: bool = False,\n) -&gt; Path:\n    \"\"\"\n    Download the dataset archive to a local directory and return the archive path.\n    Skips download if the target file already exists (unless `overwrite_existing=True`).\n\n    Automatically resumes interrupted downloads if a matching .checksum file exists from a\n    previous attempt.\n\n    Args:\n        dataset_id: The dataset ID (as shown in MDC platform).\n        download_directory: Directory where to save the downloaded archive file.\n            If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.\n        show_progress: Whether to show a progress bar during download.\n        overwrite_existing: Whether to overwrite the existing archive file.\n\n    Returns:\n        Path to the downloaded dataset archive.\n\n    Raises:\n        ValueError: If dataset_id is empty.\n        FileNotFoundError: If the dataset does not exist (404).\n        PermissionError: If access is denied (403) or download directory is not writable.\n        RuntimeError: If rate limit is exceeded (429) or unexpected response format.\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    download_plan = get_download_plan(dataset_id, download_directory)\n\n    # Case 1: Skip download if complete dataset archive already exists\n    if download_plan.target_filepath.exists() and not overwrite_existing:\n        logger.info(\n            f\"File already exists. \"\n            f\"Skipping download: `{str(download_plan.target_filepath)}`\"\n        )\n        return Path(download_plan.target_filepath)\n\n    # If overwriting, clean up any existing complete or partial download files\n    if overwrite_existing:\n        cleanup_partial_download(\n            download_plan.tmp_filepath, download_plan.checksum_filepath\n        )\n        if download_plan.target_filepath.exists():\n            download_plan.target_filepath.unlink()\n\n    # Determine whether to resume download based on existing .checksum and .part files\n    resume_checksum = determine_resume_state(download_plan)\n\n    # Write checksum file before starting download (for potential resume later)\n    if download_plan.checksum and not resume_checksum:\n        write_checksum_file(download_plan.checksum_filepath, download_plan.checksum)\n\n    execute_download_plan(download_plan, resume_checksum, show_progress)\n\n    # Download complete. Rename temp file to target and remove checksum file\n    download_plan.tmp_filepath.replace(download_plan.target_filepath)\n    if download_plan.checksum_filepath.exists():\n        download_plan.checksum_filepath.unlink()\n\n    logger.info(f\"Saved dataset to `{str(download_plan.target_filepath)}`\")\n    return Path(download_plan.target_filepath)\n</code></pre>"},{"location":"api/#datacollective.download","title":"<code>datacollective.download</code>","text":""},{"location":"api/#datacollective.download.cleanup_partial_download","title":"<code>cleanup_partial_download(tmp_filepath, checksum_filepath)</code>","text":"<p>Remove partial download files (.part and .checksum).</p> Source code in <code>src/datacollective/download.py</code> <pre><code>def cleanup_partial_download(tmp_filepath: Path, checksum_filepath: Path) -&gt; None:\n    \"\"\"Remove partial download files (.part and .checksum).\"\"\"\n    if tmp_filepath.exists():\n        tmp_filepath.unlink()\n    if checksum_filepath.exists():\n        checksum_filepath.unlink()\n</code></pre>"},{"location":"api/#datacollective.download.determine_resume_state","title":"<code>determine_resume_state(download_plan)</code>","text":"<p>Determine whether to resume a download based on existing files.</p> Cases handled <p>Case 1: .checksum and .part exist, checksum matches -&gt; resume download. Case 2: .checksum and .part exist, checksum does NOT match -&gt; start fresh. Case 3: .part exists but no .checksum -&gt; start fresh (cannot safely resume). Case 4: .checksum exists but no .part -&gt; start fresh (orphaned checksum). Case 5: Neither .checksum nor .part exist -&gt; start fresh.</p> <p>Parameters:</p> Name Type Description Default <code>download_plan</code> <code>DownloadPlan</code> <p>The DownloadPlan object with download details.</p> required <p>Returns:</p> Name Type Description <code>resume_checksum</code> <code>str | None</code> <p>The checksum to use for resumption, or None if starting fresh.</p> Source code in <code>src/datacollective/download.py</code> <pre><code>def determine_resume_state(download_plan: DownloadPlan) -&gt; str | None:\n    \"\"\"\n    Determine whether to resume a download based on existing files.\n\n    Cases handled:\n        Case 1: .checksum and .part exist, checksum matches -&gt; resume download.\n        Case 2: .checksum and .part exist, checksum does NOT match -&gt; start fresh.\n        Case 3: .part exists but no .checksum -&gt; start fresh (cannot safely resume).\n        Case 4: .checksum exists but no .part -&gt; start fresh (orphaned checksum).\n        Case 5: Neither .checksum nor .part exist -&gt; start fresh.\n\n    Args:\n        download_plan: The DownloadPlan object with download details.\n\n    Returns:\n        resume_checksum: The checksum to use for resumption, or None if starting fresh.\n    \"\"\"\n    tmp_filepath = download_plan.tmp_filepath\n\n    # Check existence of .part and .checksum files\n    part_exists = tmp_filepath.exists()\n    checksum_file_exists = download_plan.checksum_filepath.exists()\n    stored_checksum = (\n        _read_checksum_file(download_plan.checksum_filepath)\n        if checksum_file_exists\n        else None\n    )\n\n    # Case 1: Both .part and .checksum exist\n    if part_exists and checksum_file_exists and stored_checksum:\n        if stored_checksum == download_plan.checksum:\n            # Checksum matches -&gt; resume download\n            logger.info(\"Resuming previously interrupted download...\")\n            return stored_checksum\n        else:\n            # Case 2: Checksum does not match, i.e. dataset was updated -&gt; start fresh\n            logger.info(\n                \"Dataset has been updated since the previous download attempt. \"\n                \"Starting fresh download...\"\n            )\n            cleanup_partial_download(tmp_filepath, download_plan.checksum_filepath)\n            return None\n\n    # Case 3: .part exists but no .checksum: cannot safely resume -&gt; start fresh\n    if part_exists and not checksum_file_exists:\n        logger.warning(\n            \"Partial download found without checksum file. Starting fresh download...\"\n        )\n        cleanup_partial_download(tmp_filepath, download_plan.checksum_filepath)\n        return None\n\n    # Case 4: .checksum exists but no .part -&gt; start fresh\n    if checksum_file_exists and not part_exists:\n        cleanup_partial_download(tmp_filepath, download_plan.checksum_filepath)\n        return None\n\n    # Case 5: Neither .checksum nor .part exist -&gt; start fresh\n    return None\n</code></pre>"},{"location":"api/#datacollective.download.execute_download_plan","title":"<code>execute_download_plan(download_plan, resume_download_checksum, show_progress)</code>","text":"<p>Execute the download plan, downloading the dataset to a temporary path.</p> <p>Parameters:</p> Name Type Description Default <code>download_plan</code> <code>DownloadPlan</code> <p>The DownloadPlan object with download details.</p> required <code>resume_download_checksum</code> <code>str | None</code> <p>Provide the checksum to resume a previously interrupted download.</p> required <code>show_progress</code> <code>bool</code> <p>Whether to show a progress bar during download.</p> required <p>Raises:</p> Type Description <code>DownloadError</code> <p>If the download fails or is interrupted.</p> Source code in <code>src/datacollective/download.py</code> <pre><code>def execute_download_plan(\n    download_plan: DownloadPlan,\n    resume_download_checksum: str | None,\n    show_progress: bool,\n) -&gt; None:\n    \"\"\"\n    Execute the download plan, downloading the dataset to a temporary path.\n\n    Args:\n        download_plan: The DownloadPlan object with download details.\n        resume_download_checksum: Provide the checksum to resume a previously interrupted download.\n        show_progress: Whether to show a progress bar during download.\n\n    Raises:\n        DownloadError: If the download fails or is interrupted.\n    \"\"\"\n\n    headers, downloaded_bytes_so_far = _prepare_download_headers(\n        download_plan.tmp_filepath, resume_download_checksum\n    )\n\n    progress_bar = None\n    session_downloaded_bytes = 0\n    total_downloaded_bytes = downloaded_bytes_so_far\n    logger.info(f\"Downloading dataset: {download_plan.filename}\")\n    if show_progress:\n        progress_bar = ProgressBar(download_plan.size_bytes)\n        progress_bar.update(downloaded_bytes_so_far)\n        progress_bar._display()\n    try:\n        with send_api_request(\n            method=\"GET\",\n            url=download_plan.download_url,\n            stream=True,\n            timeout=HTTP_TIMEOUT,\n            extra_headers=headers,\n            include_auth_headers=False,  # Download URL is pre-signed, no auth needed\n        ) as response:\n            with open(download_plan.tmp_filepath, \"ab\") as f:\n                # Iterate over response in 64KB chunks to avoid using too much memory\n                for chunk in response.iter_content(chunk_size=1 &lt;&lt; 16):\n                    if not chunk:\n                        continue\n                    f.write(chunk)\n                    downloaded_bytes_so_far = len(chunk)\n                    session_downloaded_bytes += downloaded_bytes_so_far\n                    total_downloaded_bytes += downloaded_bytes_so_far\n                    if progress_bar:\n                        progress_bar.update(downloaded_bytes_so_far)\n\n            if progress_bar:\n                progress_bar.finish()\n    except (Exception, KeyboardInterrupt) as e:\n        raise DownloadError(\n            session_bytes=session_downloaded_bytes,\n            total_downloaded_bytes=total_downloaded_bytes,\n            total_archive_bytes=download_plan.size_bytes,\n            checksum=download_plan.checksum,\n        ) from e\n</code></pre>"},{"location":"api/#datacollective.download.get_download_plan","title":"<code>get_download_plan(dataset_id, download_directory)</code>","text":"<p>Send a POST request to the API to receive the download session details for a dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_id</code> <code>str</code> <p>The dataset ID (as shown in MDC platform).</p> required <code>download_directory</code> <code>str | None</code> <p>Directory where to save the downloaded dataset. If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.</p> required <p>Returns:</p> Type Description <code>DownloadPlan</code> <p>a DownloadPlan containing:</p> <code>DownloadPlan</code> <ul> <li>a download session URL created by the API</li> </ul> <code>DownloadPlan</code> <ul> <li>the filename for the dataset archive defined by the API</li> </ul> <code>DownloadPlan</code> <ul> <li>the final target filepath on disk where the archive will be saved</li> </ul> <code>DownloadPlan</code> <ul> <li>a temporary path for atomic download</li> </ul> <code>DownloadPlan</code> <ul> <li>the size of the dataset archive in bytes</li> </ul> <code>DownloadPlan</code> <ul> <li>the checksum of the dataset</li> </ul> <p>Raises:</p> Type Description <code>ValueError</code> <p>If dataset_id is empty.</p> <code>FileNotFoundError</code> <p>If the dataset does not exist (404).</p> <code>PermissionError</code> <p>If access is denied (403) or download directory is not writable.</p> <code>RuntimeError</code> <p>If rate limit is exceeded (429) or unexpected response format.</p> <code>HTTPError</code> <p>For other non-2xx responses.</p> Source code in <code>src/datacollective/download.py</code> <pre><code>def get_download_plan(dataset_id: str, download_directory: str | None) -&gt; DownloadPlan:\n    \"\"\"\n    Send a POST request to the API to receive the download session details for a dataset.\n\n    Args:\n        dataset_id: The dataset ID (as shown in MDC platform).\n        download_directory: Directory where to save the downloaded dataset.\n            If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.\n\n    Returns:\n        a DownloadPlan containing:\n        - a download session URL created by the API\n        - the filename for the dataset archive defined by the API\n        - the final target filepath on disk where the archive will be saved\n        - a temporary path for atomic download\n        - the size of the dataset archive in bytes\n        - the checksum of the dataset\n\n    Raises:\n        ValueError: If dataset_id is empty.\n        FileNotFoundError: If the dataset does not exist (404).\n        PermissionError: If access is denied (403) or download directory is not writable.\n        RuntimeError: If rate limit is exceeded (429) or unexpected response format.\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    if not dataset_id or not dataset_id.strip():\n        raise ValueError(\"`dataset_id` must be a non-empty string\")\n\n    base_dir = resolve_download_dir(download_directory)\n\n    # Create a download session to get `downloadUrl` and `filename`\n    session_url = f\"{_get_api_url()}/datasets/{dataset_id}/download\"\n    resp = send_api_request(method=\"POST\", url=session_url)\n\n    payload: dict[str, Any] = dict(resp.json())\n    download_url = payload.get(\"downloadUrl\")\n    filename = payload.get(\"filename\")\n    size_bytes = payload.get(\"sizeBytes\")\n    checksum = payload.get(\"checksum\")\n\n    if not download_url or not filename or not size_bytes:\n        raise RuntimeError(f\"Unexpected response format: {payload}\")\n\n    target_filepath = base_dir / filename\n\n    # Stream download to a temporary file for atomicity\n    tmp_filepath = target_filepath.with_name(target_filepath.name + \".part\")\n\n    checksum_filepath = _get_checksum_filepath(target_filepath)\n\n    download_plan = DownloadPlan(\n        download_url=download_url,\n        filename=filename,\n        target_filepath=target_filepath,\n        tmp_filepath=tmp_filepath,\n        size_bytes=int(size_bytes),\n        checksum=checksum,\n        checksum_filepath=checksum_filepath,\n    )\n    logger.debug(\n        f\"Download plan: filename={filename}, size={int(size_bytes)} bytes, target={target_filepath}\",\n    )\n    return download_plan\n</code></pre>"},{"location":"api/#datacollective.download.resolve_download_dir","title":"<code>resolve_download_dir(download_directory)</code>","text":"<p>Resolve and ensure the download directory exists and is writable.</p> <p>Parameters:</p> Name Type Description Default <code>download_directory</code> <code>str | None</code> <p>User-specified download directory. If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>The resolved Path object for the download directory.</p> Source code in <code>src/datacollective/download.py</code> <pre><code>def resolve_download_dir(download_directory: str | None) -&gt; Path:\n    \"\"\"\n    Resolve and ensure the download directory exists and is writable.\n\n    Args:\n        download_directory (str | None): User-specified download directory.\n            If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.\n\n    Returns:\n        The resolved Path object for the download directory.\n    \"\"\"\n    if download_directory and download_directory.strip():\n        base = download_directory\n    else:\n        base = os.getenv(ENV_DOWNLOAD_PATH, \"~/.mozdata/datasets\")\n    p = Path(os.path.expanduser(base))\n    p.mkdir(parents=True, exist_ok=True)\n    if not os.access(p, os.W_OK):\n        raise PermissionError(f\"Directory `{str(p)}` is not writable\")\n    logger.debug(f\"Resolved download directory: {p}\")\n    return p\n</code></pre>"},{"location":"api/#datacollective.download.write_checksum_file","title":"<code>write_checksum_file(checksum_filepath, checksum)</code>","text":"<p>Write the checksum to the .checksum file.</p> Source code in <code>src/datacollective/download.py</code> <pre><code>def write_checksum_file(checksum_filepath: Path, checksum: str) -&gt; None:\n    \"\"\"Write the checksum to the .checksum file.\"\"\"\n    checksum_filepath.write_text(checksum)\n</code></pre>"},{"location":"api/#datacollective.api_utils","title":"<code>datacollective.api_utils</code>","text":""},{"location":"api/#datacollective.api_utils.send_api_request","title":"<code>send_api_request(method, url, stream=False, extra_headers=None, timeout=HTTP_TIMEOUT, include_auth_headers=True)</code>","text":"<p>Send an HTTP request to the MDC API with appropriate headers and error handling.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>HTTP method (e.g., 'GET', 'POST').</p> required <code>url</code> <code>str</code> <p>Full URL for the API endpoint.</p> required <code>stream</code> <code>bool</code> <p>Whether to stream the response (default: False).</p> <code>False</code> <code>extra_headers</code> <code>dict[str, str] | None</code> <p>Additional headers to include in the request (default: None). E.g. for resuming</p> <code>None</code> <code>timeout</code> <code>tuple[int, int] | None</code> <p>A tuple specifying (connect timeout, read timeout) in seconds (default: None).</p> <code>HTTP_TIMEOUT</code> <code>include_auth_headers</code> <code>bool</code> <p>Whether to include authentication (API KEY) headers (default: True).</p> <code>True</code> <p>Returns:</p> Type Description <code>Response</code> <p>The HTTP response object.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the resource is not found (404).</p> <code>PermissionError</code> <p>If access is denied (403).</p> <code>RuntimeError</code> <p>If rate limit is exceeded (429).</p> <code>ValueError</code> <p>If API key is missing when authentication is required.</p> <code>HTTPError</code> <p>For other non-2xx responses.</p> Source code in <code>src/datacollective/api_utils.py</code> <pre><code>def send_api_request(\n    method: str,\n    url: str,\n    stream: bool = False,\n    extra_headers: dict[str, str] | None = None,\n    timeout: tuple[int, int] | None = HTTP_TIMEOUT,\n    include_auth_headers: bool = True,\n) -&gt; requests.Response:\n    \"\"\"\n    Send an HTTP request to the MDC API with appropriate headers and error handling.\n\n    Args:\n        method: HTTP method (e.g., 'GET', 'POST').\n        url: Full URL for the API endpoint.\n        stream: Whether to stream the response (default: False).\n        extra_headers: Additional headers to include in the request (default: None). E.g. for resuming\n        timeout: A tuple specifying (connect timeout, read timeout) in seconds (default: None).\n        include_auth_headers: Whether to include authentication (API KEY) headers (default: True).\n\n    Returns:\n        The HTTP response object.\n\n    Raises:\n        FileNotFoundError: If the resource is not found (404).\n        PermissionError: If access is denied (403).\n        RuntimeError: If rate limit is exceeded (429).\n        ValueError: If API key is missing when authentication is required.\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    headers = {\"User-Agent\": _get_user_agent()}\n    if include_auth_headers:\n        headers.update(_auth_headers())\n    if extra_headers:\n        headers.update(extra_headers)\n\n    logger.debug(f\"API request: {method.upper()} {url} (stream={stream})\")\n\n    resp = requests.request(\n        method=method.upper(),\n        url=url,\n        stream=stream,\n        headers=headers,\n        timeout=timeout,\n    )\n\n    if resp.status_code == 404:\n        raise FileNotFoundError(\"Dataset not found\")\n    if resp.status_code == 403:\n        raise PermissionError(\n            \"Access denied. Private dataset requires organization membership\"\n        )\n    if resp.status_code == 429:\n        raise RuntimeError(RATE_LIMIT_ERROR)\n    resp.raise_for_status()\n\n    return resp\n</code></pre>"},{"location":"api/#datacollective.dataset_loading_scripts.registry","title":"<code>datacollective.dataset_loading_scripts.registry</code>","text":""},{"location":"api/#datacollective.dataset_loading_scripts.registry.load_dataset_from_name_as_dataframe","title":"<code>load_dataset_from_name_as_dataframe(dataset_name, extract_dir)</code>","text":"<p>In order to enable loading MDC datasets as Pandas DataFrames, this function routes the loading process to the appropriate dataset-specific loader based on the dataset name. Each dataset loader is implemented in its own module under <code>datacollective.dataset_loading_scripts</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>The name of the dataset (lowercased).</p> required <code>extract_dir</code> <code>Path</code> <p>The directory where the dataset has been extracted.</p> required <p>Returns:     A pandas DataFrame containing the loaded dataset. Raises:     ValueError: If the dataset name is not supported for loading.</p> Source code in <code>src/datacollective/dataset_loading_scripts/registry.py</code> <pre><code>def load_dataset_from_name_as_dataframe(\n    dataset_name: str, extract_dir: Path\n) -&gt; pd.DataFrame:\n    \"\"\"\n    In order to enable loading MDC datasets as Pandas DataFrames, this function\n    routes the loading process to the appropriate dataset-specific loader based on\n    the dataset name. Each dataset loader is implemented in its own module under\n    `datacollective.dataset_loading_scripts`.\n\n    Args:\n        dataset_name (str): The name of the dataset (lowercased).\n        extract_dir (Path): The directory where the dataset has been extracted.\n    Returns:\n        A pandas DataFrame containing the loaded dataset.\n    Raises:\n        ValueError: If the dataset name is not supported for loading.\n    \"\"\"\n    if \"scripted\" in dataset_name:\n        logger.debug(f\"Routing dataset {dataset_name} to scripted loader\")\n        return _load_scripted(extract_dir)\n    if \"spontaneous\" in dataset_name:\n        logger.debug(f\"Routing dataset {dataset_name} to spontaneous loader\")\n        return _load_spontaneous(extract_dir)\n\n    raise ValueError(\n        f\"Dataset name `{dataset_name}` currently not supported for loading as DataFrame.\"\n    )\n</code></pre>"},{"location":"api/#datacollective.dataset_loading_scripts.common_voice","title":"<code>datacollective.dataset_loading_scripts.common_voice</code>","text":""},{"location":"release/","title":"Release Workflow","text":""},{"location":"release/#release-workflow","title":"Release Workflow","text":"<p>This project uses:</p> <ul> <li><code>bump-my-version</code> as the single source of truth for the version and updating it.</li> <li>GitHub Releases to trigger automatic publishing to PyPI via GitHub Actions.</li> </ul> <p>Publishing to PyPI happens only when a GitHub Release is created for a version tag (for example, <code>v0.1.0</code>).</p>"},{"location":"release/#prerequisites","title":"Prerequisites","text":"<p>Before doing a release, make sure you have:</p> <ul> <li>Push access to the repository.</li> <li>A local Python environment with <code>bump-my-version</code> installed:</li> </ul> <pre><code>uv tool install bump-my-version\n</code></pre> <ul> <li>An up-to-date local clone with all changes merged into <code>main</code>:</li> </ul> <pre><code>git checkout main\ngit pull origin main\n</code></pre>"},{"location":"release/#1-prepare-the-code-for-release","title":"1. Prepare the code for release","text":"<ol> <li>Make sure all desired changes are merged into <code>main</code>.</li> <li>Run tests and checks locally (adjust commands to your setup):</li> </ol> <pre><code>pre-commit run --all-files\npytest -q\n</code></pre>"},{"location":"release/#2-choose-the-version-bump","title":"2. Choose the version bump","text":"<p>Decide what type of version bump you need according to Semantic Versioning:</p> <ul> <li><code>patch</code>: bug fixes only (e.g. <code>0.0.34</code> \u2192 <code>0.0.35</code>)</li> <li><code>minor</code>: new features, backwards compatible (e.g. <code>0.0.34</code> \u2192 <code>0.1.0</code>)</li> <li><code>major</code>: breaking changes (e.g. <code>0.0.34</code> \u2192 <code>1.0.0</code>)</li> </ul> <p>Check the potential versioning paths with:</p> <pre><code>bump-my-version show-bump\n</code></pre> <p>And verify that the version you plan to release is the expected one:</p> <pre><code>bump-my-version show --increment minor new_version\n</code></pre> <p>Finally, you can run a dry run to see what will happen:</p> <pre><code>bump-my-version bump minor --dry-run -vv\n</code></pre>"},{"location":"release/#3-bump-the-version-using-bump-my-version","title":"3. Bump the version using bump-my-version","text":"<p>Run one of the following from the repository root:</p> <pre><code># choose exactly one:\nbump-my-version bump patch -vv\n# or\nbump-my-version bump minor -vv\n# or\nbump-my-version bump major -vv\n</code></pre> <p>This will update the version in <code>pyproject.toml</code> and <code>__init__.py</code>.</p>"},{"location":"release/#4-push-the-changes-and-tag","title":"4. Push the changes and tag","text":"<p>Commit the version bump and push to the <code>main</code> branch:</p> <pre><code>git add pyproject.toml src/datacollective/__init__.py uv.lock\ngit commit -m \"Bump version to X.Y.Z\"\ngit push origin main\n</code></pre> <p>Replace <code>vX.Y.Z</code> with the tag created in the previous step (for example, <code>v0.0.35</code>).</p>"},{"location":"release/#5-create-a-github-release","title":"5. Create a GitHub Release","text":"<ol> <li>Go to the repository page on GitHub.</li> <li>Open the <code>Releases</code> section.</li> <li>Click <code>Draft a new release</code>.</li> <li>Select the tag you just pushed (for example, <code>v0.0.35</code>).</li> <li>Set the release title to the same value (e.g. <code>v0.0.35</code>).</li> <li>Add release notes (high-level changes).</li> <li>Click <code>Publish release</code>.</li> </ol> <p>This will trigger the <code>Publish to PyPI</code> GitHub Actions workflow.</p>"},{"location":"release/#6-automatic-publish-to-pypi","title":"6. Automatic publish to PyPI","text":"<p>Once the GitHub Release is published:</p> <ul> <li>GitHub Actions automatically:</li> <li>Checks out the code.</li> <li>Builds the distribution.</li> <li>Uploads the package to PyPI using the <code>PYPI_API_TOKEN</code> secret.</li> </ul> <p>You can monitor the progress:</p> <ol> <li>Open the <code>Actions</code> tab in GitHub.</li> <li>Open the <code>Publish to PyPI</code> workflow run associated with your release.</li> <li>Wait for it to complete successfully.</li> </ol>"},{"location":"release/#7-verify-the-release-on-pypi","title":"7. Verify the release on PyPI","text":"<p>After the workflow succeeds:</p> <ol> <li>Go to the project page on PyPI.</li> <li>Confirm that the new version <code>X.Y.Z</code> is listed.</li> <li>Optionally install the package in a clean environment to verify:</li> </ol> <pre><code>pip install --upgrade datacollective==X.Y.Z\n</code></pre>"},{"location":"release/#notes-and-best-practices","title":"Notes and Best Practices","text":"<ul> <li> <p>Single source of truth: The version is managed only by bump-my-version. Do not manually edit the version in any file.</p> </li> <li> <p>Tag format: Always use <code>vX.Y.Z</code> tags (for example, <code>v0.0.35</code>). The <code>bump-my-version</code> configuration enforces this.</p> </li> <li> <p>Manual workflow dispatch (advanced): In rare cases, you can re-run the publish job from the <code>Actions</code> tab using <code>workflow_dispatch</code>, but normally you should always go through a GitHub Release.</p> </li> </ul>"}]}