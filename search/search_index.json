{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Mozilla Data Collective Python SDK Library","text":"<p>Welcome to the documentation for the <code>datacollective</code> Python client for the Mozilla Data Collective REST API.</p> <p>This library helps you:</p> <ul> <li>Authenticate with the Mozilla Data Collective.</li> <li>Download datasets to local storage.</li> <li>Load supported datasets into AI-friendly formats, such as pandas DataFrames.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI:</p> <pre><code>pip install datacollective\n</code></pre> <p>You can also use uv or other Python tooling as desired, as long as the package datacollective is installed in your environment.</p>"},{"location":"#getting-an-api-key","title":"Getting an API Key","text":"<p>To use the Mozilla Data Collective API, you need an API key:</p> <ol> <li>Sign in to the Mozilla Data Collective dashboard.</li> <li>Create or retrieve an API key from your account/settings page.</li> <li>Keep your key secret and do not commit it to version control.</li> </ol>"},{"location":"#configuration","title":"Configuration","text":"<p>The client reads configuration from environment variables and <code>.env</code> files.</p>"},{"location":"#environment-variables","title":"Environment variables","text":"<p>Required:</p> <ul> <li><code>MDC_API_KEY</code> - Your Mozilla Data Collective API key.</li> </ul> <p>Optional:</p> <ul> <li><code>MDC_API_URL</code> - API endpoint (defaults to the production URL).</li> <li><code>MDC_DOWNLOAD_PATH</code> - Local directory where datasets will be downloaded   (defaults to <code>~/.mozdata/datasets</code>).</li> </ul> <p>Example using environment variables directly:</p> <pre><code>export MDC_API_KEY=your-api-key-here\nexport MDC_API_URL=https://datacollective.mozillafoundation.org/api\nexport MDC_DOWNLOAD_PATH=~/.mozdata/datasets\n</code></pre>"},{"location":"#env-file","title":"<code>.env</code> file","text":"<p>The client will automatically load configuration from a <code>.env</code> file in your project root or present working directory.</p> <p>Create a file named <code>.env</code>:</p> <pre><code># MDC API Configuration\nMDC_API_KEY=your-api-key-here\nMDC_API_URL=https://datacollective.mozillafoundation.org/api\nMDC_DOWNLOAD_PATH=~/.mozdata/datasets\n</code></pre> <p>Security note: do not commit <code>.env</code> files to version control, as they contain secrets.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>IMPORTANT NOTE: Before trying to access any dataset, make sure you have thoroughly read and agreed to the specific dataset's conditions &amp; licensing terms.</p> <p>[!TIP] You can find the <code>dataset-id</code> by looking at the URL of the dataset's page on MDC platform. The ID is the unique string of characters located at the very end of the URL, after the <code>/datasets/</code> path. For example, for URL <code>https://datacollective.mozillafoundation.org/datasets/cmflnuzw6lrt9e6ui4kwcshvn</code> dataset id will be <code>cmflnuzw6lrt9e6ui4kwcshvn</code>.</p>"},{"location":"#download-a-dataset","title":"Download a dataset","text":"<p>Use <code>save_dataset_to_disk</code> to download a dataset to the configured download path:</p> <pre><code>from datacollective import save_dataset_to_disk\n\ndataset = save_dataset_to_disk(\"your-dataset-id\")\n\n# Depending on the implementation, `dataset` may contain metadata\n# about the downloaded files or a higher-level dataset object.\n</code></pre> <p>The files will be stored under <code>MDC_DOWNLOAD_PATH</code> (default <code>~/.mozdata/datasets</code>).</p>"},{"location":"#loading-and-querying-datasets","title":"Loading and Querying Datasets","text":"<p>Note: in-memory dataset loading is currently supported only for certain datasets.</p> <p>You can load supported datasets into memory and convert them to a <code>pandas</code> <code>DataFrame</code> for analysis:</p> <pre><code>from datacollective import load_dataset\n\ndataset = load_dataset(\"your-dataset-id\")\n\n# Convert to pandas\ndf = dataset.to_pandas()\n\n# Inspect available splits (e.g., train, dev, test)\nprint(dataset.splits)\n</code></pre> <p>Once loaded into a <code>DataFrame</code>, you can use standard <code>pandas</code> operations to filter, aggregate, and analyze the data.</p>"},{"location":"#get-dataset-details","title":"Get dataset details","text":"<p>You can retrieve info from the datasheet of a dataset without downloading it:</p> <pre><code>from datacollective import get_dataset_info\n\ninfo = get_dataset_info(\"your-dataset-id\")\nprint(info)\n</code></pre>"},{"location":"#api-reference","title":"API Reference","text":"<p>For a detailed API reference, see the API Reference section of the documentation.</p> <p>[!NOTE] This section is intended for maintainers of the <code>datacollective</code> library.</p>"},{"location":"#tests","title":"Tests","text":"<p>Run the full test suite: <pre><code>pytest -v\n</code></pre></p> <p>Note that the e2e tests require a valid <code>MDC_TEST_API_KEY</code> and a <code>MDC_TEST_API_URL</code> key set in your environment. Pytest will skip the live e2e tests automatically if either is missing.</p>"},{"location":"#release-workflow","title":"Release Workflow","text":"<p>Check out the Release Workflow document for details on how to publish new versions of the library to PyPI using GitHub Actions.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#datacollective.datasets","title":"<code>datacollective.datasets</code>","text":""},{"location":"api/#datacollective.datasets.get_dataset_details","title":"<code>get_dataset_details(dataset_id)</code>","text":"<p>Return dataset details from the MDC API as a dictionary. Args:     dataset_id: The dataset ID (as shown in MDC platform). Returns:     A dict with dataset details as returned by the API. Raises:     ValueError: If dataset_id is empty.     FileNotFoundError: If the dataset does not exist (404).     PermissionError: If access is denied (403).     RuntimeError: If rate limit is exceeded (429).     requests.HTTPError: For other non-2xx responses.</p> Source code in <code>src/datacollective/datasets.py</code> <pre><code>def get_dataset_details(dataset_id: str) -&gt; dict[str, Any]:\n    \"\"\"\n    Return dataset details from the MDC API as a dictionary.\n    Args:\n        dataset_id: The dataset ID (as shown in MDC platform).\n    Returns:\n        A dict with dataset details as returned by the API.\n    Raises:\n        ValueError: If dataset_id is empty.\n        FileNotFoundError: If the dataset does not exist (404).\n        PermissionError: If access is denied (403).\n        RuntimeError: If rate limit is exceeded (429).\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    if not dataset_id or not dataset_id.strip():\n        raise ValueError(\"`dataset_id` must be a non-empty string\")\n\n    url = f\"{_get_api_url()}/datasets/{dataset_id}\"\n    resp = api_request(\"GET\", url)\n    return dict(resp.json())\n</code></pre>"},{"location":"api/#datacollective.datasets.load_dataset","title":"<code>load_dataset(dataset_id, download_directory=None, show_progress=True, overwrite_existing=False)</code>","text":"<p>Download (if needed), extract, and load the dataset into a pandas DataFrame. Uses dataset <code>details['name']</code> to check in registry.py for dataset-specific loading logic. Args:     dataset_id: The dataset ID (as shown in MDC platform).     download_directory: Directory where to save the downloaded dataset.         If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.     show_progress: Whether to show a progress bar during download.     overwrite_existing: Whether to overwrite existing files. Returns:     A pandas DataFrame with the loaded dataset. Raises:     ValueError: If dataset_id is empty.     FileNotFoundError: If the dataset does not exist (404).     PermissionError: If access is denied (403) or download directory is not writable.     RuntimeError: If rate limit is exceeded (429) or unexpected response format.     requests.HTTPError: For other non-2xx responses.</p> Source code in <code>src/datacollective/datasets.py</code> <pre><code>def load_dataset(\n    dataset_id: str,\n    download_directory: str | None = None,\n    show_progress: bool = True,\n    overwrite_existing: bool = False,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Download (if needed), extract, and load the dataset into a pandas DataFrame.\n    Uses dataset `details['name']` to check in registry.py for dataset-specific loading logic.\n    Args:\n        dataset_id: The dataset ID (as shown in MDC platform).\n        download_directory: Directory where to save the downloaded dataset.\n            If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.\n        show_progress: Whether to show a progress bar during download.\n        overwrite_existing: Whether to overwrite existing files.\n    Returns:\n        A pandas DataFrame with the loaded dataset.\n    Raises:\n        ValueError: If dataset_id is empty.\n        FileNotFoundError: If the dataset does not exist (404).\n        PermissionError: If access is denied (403) or download directory is not writable.\n        RuntimeError: If rate limit is exceeded (429) or unexpected response format.\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    archive_path = save_dataset_to_disk(\n        dataset_id=dataset_id,\n        download_directory=download_directory,\n        show_progress=show_progress,\n        overwrite_existing=overwrite_existing,\n    )\n    base_dir = _resolve_download_dir(download_directory)\n    extract_dir = _extract_archive(archive_path, base_dir)\n\n    details = get_dataset_details(dataset_id)\n    dataset_name = str(details.get(\"name\", \"\")).lower()\n\n    return load_dataset_from_name_as_dataframe(dataset_name, extract_dir)\n</code></pre>"},{"location":"api/#datacollective.datasets.save_dataset_to_disk","title":"<code>save_dataset_to_disk(dataset_id, download_directory=None, show_progress=True, overwrite_existing=False)</code>","text":"<p>Download the dataset archive to a local directory and return the archive path. Skips download if the target file already exists (unless <code>overwrite_existing=True</code>). Args:     dataset_id: The dataset ID (as shown in MDC platform).     download_directory: Directory where to save the downloaded dataset.         If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.     show_progress: Whether to show a progress bar during download.     overwrite_existing: Whether to overwrite existing files. Returns:     Path to the downloaded dataset archive. Raises:     ValueError: If dataset_id is empty.     FileNotFoundError: If the dataset does not exist (404).     PermissionError: If access is denied (403) or download directory is not writable.     RuntimeError: If rate limit is exceeded (429) or unexpected response format.     requests.HTTPError: For other non-2xx responses.</p> Source code in <code>src/datacollective/datasets.py</code> <pre><code>def save_dataset_to_disk(\n    dataset_id: str,\n    download_directory: str | None = None,\n    show_progress: bool = True,\n    overwrite_existing: bool = False,\n) -&gt; Path:\n    \"\"\"\n    Download the dataset archive to a local directory and return the archive path.\n    Skips download if the target file already exists (unless `overwrite_existing=True`).\n    Args:\n        dataset_id: The dataset ID (as shown in MDC platform).\n        download_directory: Directory where to save the downloaded dataset.\n            If None or empty, falls back to env MDC_DOWNLOAD_PATH or default.\n        show_progress: Whether to show a progress bar during download.\n        overwrite_existing: Whether to overwrite existing files.\n    Returns:\n        Path to the downloaded dataset archive.\n    Raises:\n        ValueError: If dataset_id is empty.\n        FileNotFoundError: If the dataset does not exist (404).\n        PermissionError: If access is denied (403) or download directory is not writable.\n        RuntimeError: If rate limit is exceeded (429) or unexpected response format.\n        requests.HTTPError: For other non-2xx responses.\n    \"\"\"\n    if not dataset_id or not dataset_id.strip():\n        raise ValueError(\"`dataset_id` must be a non-empty string\")\n\n    base_dir = _resolve_download_dir(download_directory)\n\n    # Create a download session to get `downloadUrl` and `filename`\n    session_url = f\"{_get_api_url()}/datasets/{dataset_id}/download\"\n    resp = api_request(\"POST\", session_url)\n    payload: dict[str, Any] = dict(resp.json())\n\n    download_url = payload.get(\"downloadUrl\")\n    filename = payload.get(\"filename\")\n    if not download_url or not filename:\n        raise RuntimeError(f\"Unexpected response format: {payload}\")\n\n    target_path = base_dir / filename\n    if target_path.exists() and not overwrite_existing:\n        print(f\"File already exists. Skipping download: `{str(target_path)}`\")\n        return Path(target_path)\n\n    # Stream download to a temporary file for atomicity\n    tmp_path = target_path.with_suffix(target_path.suffix + \".part\")\n\n    with api_request(\n        \"GET\",\n        download_url,\n        stream=True,\n        timeout=HTTP_TIMEOUT,\n    ) as r:\n        total = int(r.headers.get(\"content-length\", \"0\"))\n\n        if show_progress:\n            print(f\"Downloading dataset: {filename}\")\n            progress_bar = ProgressBar(total)\n            # Show initial progress bar with fox at the start\n            progress_bar._display()\n        else:\n            print(f\"Downloading dataset: {filename}\")\n\n        with open(tmp_path, \"wb\") as f:\n            for chunk in r.iter_content(chunk_size=1 &lt;&lt; 16):\n                if not chunk:\n                    continue\n                f.write(chunk)\n                if show_progress:\n                    progress_bar.update(len(chunk))\n\n    if show_progress:\n        progress_bar.finish()\n\n    tmp_path.replace(target_path)\n    print(f\"Saved dataset to `{str(target_path)}`\")\n    return Path(target_path)\n</code></pre>"},{"location":"api/#datacollective.api_utils","title":"<code>datacollective.api_utils</code>","text":""},{"location":"api/#datacollective.api_utils.api_request","title":"<code>api_request(method, url, *, headers=None, timeout=None, raise_known_errors=True, **kwargs)</code>","text":"<p>Send an HTTP request with default MDC auth headers and timeout, and normalize common error codes (403/404/429) to exceptions.</p> Source code in <code>src/datacollective/api_utils.py</code> <pre><code>def api_request(\n    method: str,\n    url: str,\n    *,\n    headers: dict[str, str] | None = None,\n    timeout: tuple[int, int] | None = None,\n    raise_known_errors: bool = True,\n    **kwargs: Any,\n) -&gt; requests.Response:\n    \"\"\"\n    Send an HTTP request with default MDC auth headers and timeout, and\n    normalize common error codes (403/404/429) to exceptions.\n    \"\"\"\n    merged_headers = {**_auth_headers(), **(headers or {})}\n    resp = requests.request(\n        method=method.upper(),\n        url=url,\n        headers=merged_headers,\n        timeout=HTTP_TIMEOUT if timeout is None else timeout,\n        **kwargs,\n    )\n\n    if raise_known_errors:\n        if resp.status_code == 404:\n            raise FileNotFoundError(\"Dataset not found\")\n        if resp.status_code == 403:\n            raise PermissionError(\n                \"Access denied. Private dataset requires organization membership\"\n            )\n        if resp.status_code == 429:\n            raise RuntimeError(RATE_LIMIT_ERROR)\n        resp.raise_for_status()\n\n    return resp\n</code></pre>"},{"location":"api/#datacollective.dataset_loading_scripts.registry","title":"<code>datacollective.dataset_loading_scripts.registry</code>","text":""},{"location":"api/#datacollective.dataset_loading_scripts.registry.load_dataset_from_name_as_dataframe","title":"<code>load_dataset_from_name_as_dataframe(dataset_name, extract_dir)</code>","text":"<p>In order to enable loading MDC datasets as Pandas DataFrames, this function routes the loading process to the appropriate dataset-specific loader based on the dataset name. Each dataset loader is implemented in its own module under <code>datacollective.dataset_loading_scripts</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_name</code> <code>str</code> <p>The name of the dataset (lowercased).</p> required <code>extract_dir</code> <code>Path</code> <p>The directory where the dataset has been extracted.</p> required <p>Returns:     A pandas DataFrame containing the loaded dataset. Raises:     ValueError: If the dataset name is not supported for loading.</p> Source code in <code>src/datacollective/dataset_loading_scripts/registry.py</code> <pre><code>def load_dataset_from_name_as_dataframe(\n    dataset_name: str, extract_dir: Path\n) -&gt; pd.DataFrame:\n    \"\"\"\n    In order to enable loading MDC datasets as Pandas DataFrames, this function\n    routes the loading process to the appropriate dataset-specific loader based on\n    the dataset name. Each dataset loader is implemented in its own module under\n    `datacollective.dataset_loading_scripts`.\n\n    Args:\n        dataset_name (str): The name of the dataset (lowercased).\n        extract_dir (Path): The directory where the dataset has been extracted.\n    Returns:\n        A pandas DataFrame containing the loaded dataset.\n    Raises:\n        ValueError: If the dataset name is not supported for loading.\n    \"\"\"\n    if \"scripted\" in dataset_name:\n        return _load_scripted(extract_dir)\n    if \"spontaneous\" in dataset_name:\n        return _load_spontaneous(extract_dir)\n\n    raise ValueError(\n        f\"Dataset name `{dataset_name}` currently not supported for loading as DataFrame.\"\n    )\n</code></pre>"},{"location":"api/#datacollective.dataset_loading_scripts.common_voice","title":"<code>datacollective.dataset_loading_scripts.common_voice</code>","text":""},{"location":"release/","title":"Release Workflow","text":""},{"location":"release/#release-workflow","title":"Release Workflow","text":"<p>This project uses:</p> <ul> <li><code>pyproject.toml</code> as the single source of truth for the version.</li> <li><code>bump2version</code> to bump the version and create Git tags.</li> <li>GitHub Releases to trigger automatic publishing to PyPI via GitHub Actions.</li> </ul> <p>Publishing to PyPI happens only when a GitHub Release is created for a version tag (for example, <code>v0.1.0</code>).</p>"},{"location":"release/#prerequisites","title":"Prerequisites","text":"<p>Before doing a release, make sure you have:</p> <ul> <li>Push access to the main repository.</li> <li>A local Python environment with <code>bump2version</code> installed:</li> </ul> <pre><code>pip install bump2version\n</code></pre> <ul> <li>An up-to-date local clone:</li> </ul> <pre><code>git checkout main\ngit pull origin main\n</code></pre>"},{"location":"release/#1-prepare-the-code-for-release","title":"1. Prepare the code for release","text":"<ol> <li>Make sure all desired changes are merged into <code>main</code>.</li> <li>Run tests and checks locally (adjust commands to your setup):</li> </ol> <pre><code>pre-commit run --all-files\npytest -q\n</code></pre>"},{"location":"release/#2-choose-the-version-bump","title":"2. Choose the version bump","text":"<p>Decide what type of version bump you need according to Semantic Versioning:</p> <ul> <li><code>patch</code>: bug fixes only (e.g. <code>0.0.34</code> \u2192 <code>0.0.35</code>)</li> <li><code>minor</code>: new features, backwards compatible (e.g. <code>0.0.34</code> \u2192 <code>0.1.0</code>)</li> <li><code>major</code>: breaking changes (e.g. <code>0.0.34</code> \u2192 <code>1.0.0</code>)</li> </ul>"},{"location":"release/#3-bump-the-version-using-bump2version","title":"3. Bump the version using bump2version","text":"<p>Run one of the following from the repository root:</p> <pre><code># choose exactly one:\nbump2version patch\n# or\nbump2version minor\n# or\nbump2version major\n</code></pre> <p>What this does:</p> <ul> <li>Updates the version in <code>pyproject.toml</code>.</li> <li>Commits the version bump.</li> <li>Creates a Git tag <code>vX.Y.Z</code> matching the new version.</li> </ul>"},{"location":"release/#4-push-the-changes-and-tag","title":"4. Push the changes and tag","text":"<p>Push the new commit and the created tag to GitHub:</p> <pre><code>git push origin main\ngit push origin vX.Y.Z\n</code></pre> <p>Replace <code>vX.Y.Z</code> with the tag created in the previous step (for example, <code>v0.0.35</code>).</p>"},{"location":"release/#5-create-a-github-release","title":"5. Create a GitHub Release","text":"<ol> <li>Go to the repository page on GitHub.</li> <li>Open the <code>Releases</code> section.</li> <li>Click <code>Draft a new release</code>.</li> <li>Select the tag you just pushed (for example, <code>v0.0.35</code>).</li> <li>Set the release title to the same value (e.g. <code>v0.0.35</code>).</li> <li>Optionally add release notes (high-level changes).</li> <li>Click <code>Publish release</code>.</li> </ol> <p>This will trigger the <code>Publish to PyPI</code> GitHub Actions workflow.</p>"},{"location":"release/#6-automatic-publish-to-pypi","title":"6. Automatic publish to PyPI","text":"<p>Once the GitHub Release is published:</p> <ul> <li>GitHub Actions automatically:</li> <li>Checks out the code.</li> <li>Builds the distribution.</li> <li>Uploads the package to PyPI using the <code>PYPI_API_TOKEN</code> secret.</li> </ul> <p>You can monitor the progress:</p> <ol> <li>Open the <code>Actions</code> tab in GitHub.  </li> <li>Open the <code>Publish to PyPI</code> workflow run associated with your release.  </li> <li>Wait for it to complete successfully.</li> </ol>"},{"location":"release/#7-verify-the-release-on-pypi","title":"7. Verify the release on PyPI","text":"<p>After the workflow succeeds:</p> <ol> <li>Go to the project page on PyPI.</li> <li>Confirm that the new version <code>X.Y.Z</code> is listed.</li> <li>Optionally install the package in a clean environment to verify:</li> </ol> <pre><code>pip install --upgrade datacollective==X.Y.Z\n</code></pre>"},{"location":"release/#notes-and-best-practices","title":"Notes and Best Practices","text":"<ul> <li> <p>Single source of truth: The version is defined only in <code>pyproject.toml</code>. Do not manually edit <code>__version__</code> or other files for versioning.</p> </li> <li> <p>Tag format: Always use <code>vX.Y.Z</code> tags (for example, <code>v0.0.35</code>). The <code>bump2version</code> configuration enforces this.</p> </li> <li> <p>Manual workflow dispatch (advanced): In rare cases, you can re-run the publish job from the <code>Actions</code> tab using <code>workflow_dispatch</code>, but normally you should always go through a GitHub Release.</p> </li> </ul>"}]}